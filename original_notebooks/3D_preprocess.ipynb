{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nibabel\n",
    "import nibabel.processing\n",
    "import os\n",
    "from skimage.filters import threshold_otsu\n",
    "import cc3d\n",
    "import shutil\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (sys.version)\n",
    "print (sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE SMALL GPU#\n",
    "use_gpu = 1 \n",
    "# The largest memory size GPU is always the first one (0) as they are sorted by size!\n",
    "gpus=tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[use_gpu], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min-max scaling between 0 and 1\n",
    "def normalize(volume):\n",
    "    \"\"\"Normalize the volume\"\"\"\n",
    "    min = volume.min()\n",
    "    max = volume.max()\n",
    "    volume = (volume - min) / (max - min)\n",
    "    volume = volume.astype(\"float32\")\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is similar to Ding's preprocess, except CCA is performed in 3D and the largest label is the one we keep\n",
    "\n",
    "def pp(original_image):\n",
    "    \n",
    "    input_img=input_img = nibabel.load(\"{0}\".format(original_image))\n",
    "    resampled_img = nibabel.processing.conform(input_img, out_shape=(100,100,90), voxel_size=(2.0, 2.0, 2.0))\n",
    "    \n",
    "    img = resampled_img.get_fdata()\n",
    "    \n",
    "    thresh = threshold_otsu(img)\n",
    "    bw_img1 = np.copy(img)\n",
    "    bw_img1[bw_img1 < thresh] = 0\n",
    "    bw_img1[bw_img1 >= thresh] = 255\n",
    "    \n",
    "    input_CCA=bw_img1.astype('int32')\n",
    "    connectivity = 6\n",
    "    labels_out, N = cc3d.connected_components(input_CCA, return_N=True)\n",
    "    \n",
    "    def mask_largest_label (labels_out, N):\n",
    "        print(\"This function returns the largest blob of a CCA processed image as a binary mask\")\n",
    "        print(\"\")\n",
    "        def separate_labels(label_ID, label_matrix):\n",
    "            mask=1*(label_matrix == label_ID)\n",
    "            return mask\n",
    "        labellist=[]\n",
    "        for j in range(1, N+1):\n",
    "            a=separate_labels(j, labels_out)\n",
    "            labellist.append(a)\n",
    "        print(\"The image has {0} labels\".format(len(labellist)))\n",
    "        z=labellist[0]\n",
    "        print(\"The shape of the labels is: {0}\".format(z.shape))\n",
    "        sizelist=[]\n",
    "        for counter,element in enumerate (labellist):\n",
    "            a=labellist[counter].sum()\n",
    "            sizelist.append(a)\n",
    "        print(\"Label sizes: {0}\".format(sizelist))\n",
    "        sizelist=np.asarray(sizelist)\n",
    "        a=sizelist.argmax()\n",
    "        print(\"The largest label index is: {0}\".format(a))\n",
    "        mask=labellist[a]\n",
    "        print(\"The largest label is now a binary mask with shape {0}, size {1}, max value {2} and min value {3}\".format((mask.shape),(mask.sum()),(mask.max()),(mask.min())))\n",
    "        return mask\n",
    "\n",
    "    mask=mask_largest_label(labels_out, N)\n",
    "    \n",
    "    pimg=np.multiply(img,mask)\n",
    "    \n",
    "    return pimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapper function for normalize and preprocess\n",
    "\n",
    "def process_scan(path):\n",
    "    \"\"\"Read and normalize volume\"\"\"\n",
    "    # Read and pp scan\n",
    "    volume = pp(path)\n",
    "    # Normalize\n",
    "    volume = normalize(volume)\n",
    "    return volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a binary version of the split cell in Ding_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define base destination folders\n",
    "\n",
    "train=\"/local_mount/space/celer/1/users/notebooks/moises/pdata/newdata/split/train\"\n",
    "test=\"/local_mount/space/celer/1/users/notebooks/moises/pdata/newdata/split/test\"\n",
    "val=\"/local_mount/space/celer/1/users/notebooks/moises/pdata/newdata/split/val\"\n",
    "\n",
    "\n",
    "#Loop for AD\n",
    "\n",
    "print(\"\\n AD \\n\")\n",
    "\n",
    "os.chdir(\"/local_mount/space/celer/1/users/notebooks/moises/pdata/newdata/ad\")\n",
    "v=os.listdir()\n",
    "v.sort()\n",
    "\n",
    "val_samples=round(0.1*len(v))\n",
    "test_samples=round(0.1*len(v))\n",
    "\n",
    "for i,j in enumerate(v[0:val_samples]):\n",
    "    print(\"Este es de val\",i,j)\n",
    "    shutil.copy(j,\"{0}/ad\".format(val))\n",
    "    print(\"Copied\")\n",
    "\n",
    "ec=0\n",
    "for i,j in enumerate(v[val_samples-1:]):\n",
    "    if v[val_samples-1:][i][0:10]==v[val_samples-1:][i+1][0:10]:\n",
    "        print(\"Extra de val\",i+val_samples,v[val_samples-1:][i+1])\n",
    "        shutil.copy(\"{0}\".format(v[val_samples-1:][i+1]),\"{0}/ad\".format(val))\n",
    "        print(\"Copied\")\n",
    "        ec=ec+1\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "for i,j in enumerate(v[val_samples+ec:val_samples+ec+test_samples]):\n",
    "    print(\"Este es test\",i+val_samples+ec,j)\n",
    "    shutil.copy(j,\"{0}/ad\".format(test))\n",
    "    print(\"Copied\")\n",
    "\n",
    "eec=0\n",
    "for i,j in enumerate(v[val_samples+ec+test_samples-1:]):\n",
    "    if v[val_samples+ec+test_samples-1:][i][0:10]==v[val_samples+ec+test_samples-1:][i+1][0:10]:\n",
    "        print(\"Extra test\",i+val_samples+ec+test_samples,v[val_samples+ec+test_samples-1:][i+1])\n",
    "        shutil.copy(\"{0}\".format(v[val_samples+ec+test_samples-1:][i+1]),\"{0}/ad\".format(test))\n",
    "        eec=eec+1\n",
    "    else:\n",
    "        break\n",
    "for i,j in enumerate(v[val_samples+ec+test_samples+eec:]):\n",
    "    print(\"Este es train\",i+val_samples+ec+test_samples+eec,j)\n",
    "    shutil.copy(j,\"{0}/ad\".format(train))\n",
    "    print(\"Copied\")\n",
    "    \n",
    "\"\"\"\"\"\n",
    "#Loop for MCI\n",
    "\n",
    "print(\"\\n MCI \\n\")\n",
    "\n",
    "os.chdir(\"/local_mount/space/celer/1/users/notebooks/moises/pdata/newdata/mci\")\n",
    "v=os.listdir()\n",
    "v.sort()\n",
    "\n",
    "val_samples=round(0.1*len(v))\n",
    "test_samples=round(0.1*len(v))\n",
    "\n",
    "for i,j in enumerate(v[0:val_samples]):\n",
    "    print(\"Este es de val\",i,j)\n",
    "    shutil.copy(j,\"{0}/mci\".format(val))\n",
    "    print(\"Copied\")\n",
    "\n",
    "ec=0\n",
    "for i,j in enumerate(v[val_samples-1:]):\n",
    "    if v[val_samples-1:][i][0:10]==v[val_samples-1:][i+1][0:10]:\n",
    "        print(\"Extra de val\",i+val_samples,v[val_samples-1:][i+1])\n",
    "        shutil.copy(\"{0}\".format(v[val_samples-1:][i+1]),\"{0}/mci\".format(val))\n",
    "        print(\"Copied\")\n",
    "        ec=ec+1\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "for i,j in enumerate(v[val_samples+ec:val_samples+ec+test_samples]):\n",
    "    print(\"Este es test\",i+val_samples+ec,j)\n",
    "    shutil.copy(j,\"{0}/mci\".format(test))\n",
    "    print(\"Copied\")\n",
    "\n",
    "eec=0\n",
    "for i,j in enumerate(v[val_samples+ec+test_samples-1:]):\n",
    "    if v[val_samples+ec+test_samples-1:][i][0:10]==v[val_samples+ec+test_samples-1:][i+1][0:10]:\n",
    "        print(\"Extra test\",i+val_samples+ec+test_samples,v[val_samples+ec+test_samples-1:][i+1])\n",
    "        shutil.copy(\"{0}\".format(v[val_samples+ec+test_samples-1:][i+1]),\"{0}/mci\".format(test))\n",
    "        eec=eec+1\n",
    "    else:\n",
    "        break\n",
    "for i,j in enumerate(v[val_samples+ec+test_samples+eec:]):\n",
    "    print(\"Este es train\",i+val_samples+ec+test_samples+eec,j)\n",
    "    shutil.copy(j,\"{0}/mci\".format(train))\n",
    "    print(\"Copied\")\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "#Loop for Control\n",
    "\n",
    "print(\"\\n Control \\n\")\n",
    "\n",
    "os.chdir(\"/local_mount/space/celer/1/users/notebooks/moises/pdata/newdata/control\")\n",
    "v=os.listdir()\n",
    "v.sort()\n",
    "\n",
    "val_samples=round(0.1*len(v))\n",
    "test_samples=round(0.1*len(v))\n",
    "\n",
    "for i,j in enumerate(v[0:val_samples]):\n",
    "    print(\"Este es de val\",i,j)\n",
    "    shutil.copy(j,\"{0}/control\".format(val))\n",
    "    print(\"Copied\")\n",
    "\n",
    "ec=0\n",
    "for i,j in enumerate(v[val_samples-1:]):\n",
    "    if v[val_samples-1:][i][0:10]==v[val_samples-1:][i+1][0:10]:\n",
    "        print(\"Extra de val\",i+val_samples,v[val_samples-1:][i+1])\n",
    "        shutil.copy(\"{0}\".format(v[val_samples-1:][i+1]),\"{0}/control\".format(val))\n",
    "        print(\"Copied\")\n",
    "        ec=ec+1\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "for i,j in enumerate(v[val_samples+ec:val_samples+ec+test_samples]):\n",
    "    print(\"Este es test\",i+val_samples+ec,j)\n",
    "    shutil.copy(j,\"{0}/control\".format(test))\n",
    "    print(\"Copied\")\n",
    "\n",
    "eec=0\n",
    "for i,j in enumerate(v[val_samples+ec+test_samples-1:]):\n",
    "    if v[val_samples+ec+test_samples-1:][i][0:10]==v[val_samples+ec+test_samples-1:][i+1][0:10]:\n",
    "        print(\"Extra test\",i+val_samples+ec+test_samples,v[val_samples+ec+test_samples-1:][i+1])\n",
    "        shutil.copy(\"{0}\".format(v[val_samples+ec+test_samples-1:][i+1]),\"{0}/control\".format(test))\n",
    "        eec=eec+1\n",
    "    else:\n",
    "        break\n",
    "for i,j in enumerate(v[val_samples+ec+test_samples+eec:]):\n",
    "    print(\"Este es train\",i+val_samples+ec+test_samples+eec,j)\n",
    "    shutil.copy(j,\"{0}/control\".format(train))\n",
    "    print(\"Copied\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we create a list of all the paths to specific images (you can see different versions below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local_mount/space/celer/1/users/notebooks/moises\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_control_scan_paths = [\n",
    "    os.path.join(os.getcwd(), \"pdata/newdata/split/train/control\", x)\n",
    "    for x in os.listdir(\"pdata/newdata/split/train/control\")\n",
    "]\n",
    "\n",
    "val_control_scan_paths = [\n",
    "    os.path.join(os.getcwd(), \"pdata/newdata/split/val/control\", x)\n",
    "    for x in os.listdir(\"pdata/newdata/split/val/control\")\n",
    "]\n",
    "\n",
    "test_control_scan_paths = [\n",
    "    os.path.join(os.getcwd(), \"pdata/newdata/split/test/control\", x)\n",
    "    for x in os.listdir(\"pdata/newdata/split/test/control\")\n",
    "]\n",
    "\n",
    "train_ad_scan_paths = [\n",
    "    os.path.join(os.getcwd(), \"pdata/newdata/split/train/ad\", x)\n",
    "    for x in os.listdir(\"pdata/newdata/split/train/ad\")\n",
    "]\n",
    "\n",
    "val_ad_scan_paths = [\n",
    "    os.path.join(os.getcwd(), \"pdata/newdata/split/val/ad\", x)\n",
    "    for x in os.listdir(\"pdata/newdata/split/val/ad\")\n",
    "]\n",
    "\n",
    "test_ad_scan_paths = [\n",
    "    os.path.join(os.getcwd(), \"pdata/newdata/split/test/ad\", x)\n",
    "    for x in os.listdir(\"pdata/newdata/split/test/ad\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read and process the scans (binary AD/control))\n",
    "\n",
    "train_ad_scans = np.array([process_scan(path) for path in train_ad_scan_paths])\n",
    "val_ad_scans = np.array([process_scan(path) for path in val_ad_scan_paths])\n",
    "test_ad_scans = np.array([process_scan(path) for path in test_ad_scan_paths])\n",
    "\n",
    "train_control_scans = np.array([process_scan(path) for path in train_control_scan_paths])\n",
    "val_control_scans = np.array([process_scan(path) for path in val_control_scan_paths])\n",
    "test_control_scans = np.array([process_scan(path) for path in test_control_scan_paths])\n",
    "\n",
    "\n",
    "# # Labeling samples according to folder architecture\n",
    "train_ad_labels = np.array([1 for _ in range(len(train_ad_scans))])\n",
    "val_ad_labels = np.array([1 for _ in range(len(val_ad_scans))])\n",
    "test_ad_labels = np.array([1 for _ in range(len(test_ad_scans))])\n",
    "\n",
    "train_control_labels = np.array([0 for _ in range(len(train_control_scans))])\n",
    "val_control_labels = np.array([0 for _ in range(len(val_control_scans))])\n",
    "test_control_labels = np.array([0 for _ in range(len(test_control_scans))])\n",
    "\n",
    "#Sets\n",
    "x_train = np.concatenate((train_ad_scans,train_control_scans),axis=0)\n",
    "y_train = np.concatenate((train_ad_labels,train_control_labels), axis=0)\n",
    "\n",
    "x_val = np.concatenate((val_ad_scans,val_control_scans),axis=0)\n",
    "y_val = np.concatenate((val_ad_labels,val_control_labels), axis=0)\n",
    "\n",
    "x_test = np.concatenate((test_ad_scans,test_control_scans),axis=0)\n",
    "y_test = np.concatenate((test_ad_labels,test_control_labels), axis=0)\n",
    "\n",
    "print(\n",
    "    \"Number of samples in train and validation are %d and %d.\"\n",
    "    % (x_train.shape[0], x_val.shape[0])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train and validation are 1681 and 217.\n"
     ]
    }
   ],
   "source": [
    "#We can double check the amount of samples\n",
    "print(\n",
    "    \"Number of samples in train and validation are %d and %d.\"\n",
    "    % (x_train.shape[0], x_val.shape[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a tensorflow dataset object with all this preprocessing already done\n",
    "gtrain_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "gvalidation_loader = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "gtest_loader= tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we save the dataset to a folder (beware of the element_spec, which is needed to reload the dataset later on).\n",
    "#The name of the dataset needs to be substituted in the ### spots below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(gtrain_loader, \"./datasets/###TRAIN###\")\n",
    "\n",
    "tf.data.experimental.save(gvalidation_loader, \"./datasets/###VAL###\")\n",
    "\n",
    "tf.data.experimental.save(gtest_loader, \"./datasets/###TEST###\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We get the element_spec and then save it to a pickle object\n",
    "a=gtrain_loader.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('###Dataset###.pickle', 'wb') as f:\n",
    "    pickle.dump(a, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
